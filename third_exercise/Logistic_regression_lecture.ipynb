{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic(y):\n",
    "    out = np.exp(y)/(1+np.exp(y))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x6f7ba58>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1fL/AQe6XnmCNX6AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6eb43c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10,10,num=1000)\n",
    "plt.plot(x, logistic(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of logistic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('admission_dataset.txt')\n",
    "data_matrix = data[:,[0,1]]\n",
    "admission_labels = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    E = 0\n",
    "    for n in range(N):\n",
    "        E = E + (1/N)*np.log(1/logistic(y)*np.dot(w, X[n,:]))\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of checking our logistic log likelihood function logistic_insample, let's simulate a starting value of $w$ and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.18418489  0.18418489  0.18418489  0.18418489  1.18418489  0.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  0.18418489  0.18418489  0.18418489  0.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  0.18418489  1.18418489  0.18418489\n",
      "  0.18418489  1.18418489  1.18418489  0.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  0.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489\n",
      "  0.18418489  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  0.18418489  1.18418489\n",
      "  0.18418489  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  0.18418489\n",
      "  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  0.18418489  0.18418489  1.18418489  0.18418489  0.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  0.18418489\n",
      "  0.18418489  1.18418489  1.18418489  0.18418489  0.18418489  1.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  0.18418489  0.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  1.18418489  0.18418489  0.18418489\n",
      "  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  0.18418489  0.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  0.18418489  1.18418489  0.18418489  0.18418489  0.18418489\n",
      "  0.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  0.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489  0.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489  1.18418489\n",
      "  1.18418489  1.18418489  0.18418489  1.18418489  0.18418489  1.18418489\n",
      "  0.18418489  0.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  0.18418489  0.18418489  1.18418489  1.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489  0.18418489  0.18418489\n",
      "  0.18418489  0.18418489  1.18418489  1.18418489  1.18418489  0.18418489\n",
      "  1.18418489  1.18418489  1.18418489  0.18418489  1.18418489  1.18418489\n",
      "  0.18418489  1.18418489  0.18418489  1.18418489  1.18418489  1.18418489\n",
      "  0.18418489  0.18418489  0.18418489  0.18418489  0.18418489  1.18418489\n",
      "  1.18418489  1.18418489  1.18418489  1.18418489]\n"
     ]
    }
   ],
   "source": [
    "N, num_feat = data_matrix.shape\n",
    "#Append a column of ones to the data matrix\n",
    "onevec = np.ones((N,1))\n",
    "X = np.concatenate((onevec, data_matrix), axis = 1)\n",
    "# Let's turn the admission labels into +/- 1 labels\n",
    "y = (admission_labels-0.5)*2\n",
    "\n",
    "np.random.seed(0)\n",
    "w = 0.1*np.random.randn(num_feat + 1)\n",
    "L = logistic_insample(X, y, w)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-ef92a70b08c8>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-ef92a70b08c8>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    g = g + #pointwise gradient\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def logistic_gradient(X, y, w):\n",
    "    N, _ = X.shape\n",
    "    g = 0*w\n",
    "    \n",
    "    for n in range(N):\n",
    "        g = g + 1/N * -y[n] \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for checking the gradient function, let's evaluate it on our simulated $w$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = logistic_gradient(X,y,w)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having working functions for logistic log likelihood and logistic gradient, let's implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_reg(Xorig, y, max_iter, grad_thr):   \n",
    "    \n",
    "    # X is a d by N data matrix of input values\n",
    "    num_pts, num_feat = Xorig.shape\n",
    "    onevec = np.ones((num_pts,1))\n",
    "    X = np.concatenate((onevec, Xorig), axis = 1)\n",
    "    dplus1 = num_feat + 1\n",
    "        \n",
    "    # y is a N by 1 matrix of target values -1 and 1\n",
    "    y = np.array((y-.5)*2)\n",
    "        \n",
    "    # Initialize learning rate for gradient descent\n",
    "    learningrate = 0.1        \n",
    "    \n",
    "    # Initialize weights at time step 0    \n",
    "    w = 0.1*np.random.randn(num_feat + 1)\n",
    "    \n",
    "    # Compute value of logistic log likelihood\n",
    "    value = logistic_insample(X,y,w)\n",
    "    \n",
    "    num_iter = 0  \n",
    "    convergence = 0\n",
    "    \n",
    "    # Keep track of function values\n",
    "    E_in = []\n",
    "    \n",
    "    while convergence == 0:\n",
    "        num_iter = num_iter + 1                        \n",
    "\n",
    "        # Compute gradient at current w      \n",
    "        g = logistic_gradient(X,y,w)\n",
    "       \n",
    "        # Set direction to move and take a step       \n",
    "        \n",
    "        ##############################################################\n",
    "        \n",
    "        w_new = #####Fill in!\n",
    "        \n",
    "        #################################################################\n",
    "       \n",
    "        # Check for improvement\n",
    "        # Compute in-sample error for new w\n",
    "        cur_value = logistic_insample(X,y,w_new)\n",
    "        if cur_value < value:\n",
    "            w = w_new\n",
    "            value = cur_value\n",
    "            E_in.append(value)\n",
    "            learningrate *=1.1\n",
    "        else:\n",
    "            learningrate *= 0.9   \n",
    "            \n",
    "        # Determine whether we have converged: Is gradient norm below\n",
    "        # threshold, and have we reached max_iter?\n",
    "               \n",
    "        g_norm = np.linalg.norm(g)\n",
    "        if g_norm < grad_thr:\n",
    "            convergence = 1\n",
    "        elif num_iter > max_iter:\n",
    "            convergence = 1\n",
    "           \n",
    "    return w, E_in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, E = log_reg(data_matrix, y, 20000, 0.0000)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Problem: Can you make the code faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_pred(Xorig, w):\n",
    "    # Fill in\n",
    "    return P, pred_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the classification accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P, pred_classes = log_pred(data_matrix, w)\n",
    "errors = np.sum(np.abs(pred_classes - y)/2)\n",
    "error_rate = errors/N\n",
    "print(error_rate, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on? Let's try with a weight matrix $w = [-4.9494, 0.7547, 0.2691]$ obtained by waiting much longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P, pred_classes = log_pred(data_matrix, [-4.9494, 0.7547, 0.2691])\n",
    "errors = np.sum(np.abs(pred_classes - y)/2)\n",
    "error_rate = errors/N\n",
    "print(error_rate, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My classification accuracy is not great -- is this just not working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
